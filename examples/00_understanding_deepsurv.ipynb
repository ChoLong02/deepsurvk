{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we will change a few settings to make the notebook look a bit prettier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> body {font-family: \"Calibri\", cursive, sans-serif;} </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style> body {font-family: \"Calibri\", cursive, sans-serif;} </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/IKNL/guidelines/blob/master/resources/logos/iknl_nl.png?raw=true\" width=200 align=\"right\">\n",
    "\n",
    "# 00 - DeepSurv Using Keras\n",
    "Originally, DeepSurv is implemented in Theano (using Lasagne).\n",
    "However, [Theano is no longer supported](https://groups.google.com/forum/#!msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ).\n",
    "Therefore, I thought it would be a good idea to do my own implementation\n",
    "of it using Keras. Not only I think this will give increase DeepSurv's use,\n",
    "but it is also a good learning exercise for myself. \n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, ActivityRegularization\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import lifelines\n",
    "from lifelines import datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import h5py\n",
    "\n",
    "import logzero\n",
    "from logzero import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logzero.logfile(\"./logfile.log\", maxBytes=1e6, backupCount=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "We will use the [Rotterdam & German Breast Cancer Study Group (GBSG) dataset](https://ascopubs.org/doi/abs/10.1200/jco.1994.12.10.2086).\n",
    "The dataset was fetched from the [czifan's `DeepSurv.pytorch` repository](https://github.com/czifan/DeepSurv.pytorch)\n",
    "(an attempt to implement DeepSurv in yet another platform, in this case\n",
    "PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rossi_dataset = datasets.load_rossi()\n",
    "E = np.array(rossi_dataset[\"arrest\"])\n",
    "Y = np.array(rossi_dataset[\"week\"])\n",
    "X = np.array(rossi_dataset)\n",
    "X = X.astype('float64')\n",
    "X = X[:,2:]\n",
    "\n",
    "n_patients = X.shape[0]\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "logger.info(\"Partitioning data...\")\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=0)\n",
    "X_train, X_val, E_train, E_val = train_test_split(X, E, test_size=0.25, random_state=0)\n",
    "logger.info(\"DONE!\")\n",
    "\n",
    "\n",
    "#Standardize\n",
    "scaler = StandardScaler().fit(X_train[:,[1,6]])\n",
    "\n",
    "X_train[:, [1,6]] = scaler.transform(X_train[:, [1,6]])\n",
    "X_val[:, [1,6]] = scaler.transform(X_val[:, [1,6]])\n",
    "\n",
    "#Sorting for NNL!\n",
    "sort_idx = np.argsort(Y_train)[::-1]\n",
    "X_train = X_train[sort_idx]\n",
    "Y_train = Y_train[sort_idx]\n",
    "E_train = E_train[sort_idx]\n",
    "\n",
    "n_patients_train = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Function\n",
    "def negative_log_likelihood(E):\n",
    "    def loss(y_true, y_pred):\n",
    "        # print(\"Y true\")\n",
    "        # print(y_true)\n",
    "        # tf.print(y_true)\n",
    "        \n",
    "        print(\"Y pred\")\n",
    "        tf.print(y_pred)\n",
    "        print(tf.shape(y_pred))\n",
    "        \n",
    "        hazard_ratio = tf.math.exp(y_pred)\n",
    "        # print(\"Hazard ratio\")\n",
    "        # tf.print(hazard_ratio)\n",
    "        \n",
    "        # print(\"log risk\")\n",
    "        log_risk = tf.math.log(tf.math.cumsum(hazard_ratio))\n",
    "        # tf.print(log_risk)\n",
    "        \n",
    "        # print(\"uncensored likelihood\")\n",
    "        uncensored_likelihood = tf.transpose(y_pred) - log_risk\n",
    "        # tf.print(uncensored_likelihood)\n",
    "        \n",
    "        # print(\"censored likelihood\")\n",
    "        censored_likelihood = uncensored_likelihood * E\n",
    "        # tf.print(censored_likelihood)\n",
    "        \n",
    "        # print(\"num observed events\")\n",
    "        num_observed_events = tf.math.cumsum(E)\n",
    "        num_observed_events = tf.cast(num_observed_events, dtype=tf.float32)\n",
    "        # tf.print(num_observed_events)\n",
    "        \n",
    "        num_observed_events = tf.constant(1, dtype=tf.float32)\n",
    "        \n",
    "        neg_likelihood_ = -tf.math.reduce_sum(censored_likelihood)\n",
    "        neg_likelihood = neg_likelihood_ / num_observed_events\n",
    "        # tf.print(neg_likelihood)\n",
    "        \n",
    "        return neg_likelihood\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='relu', kernel_initializer='glorot_uniform', input_shape=(n_features,))) # shape = length, dimension\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=32, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=1, activation=\"linear\", kernel_initializer='glorot_uniform', kernel_regularizer=l2(0.01)))\n",
    "model.add(ActivityRegularization(l2=0.01))\n",
    "\n",
    "sgd = SGD(lr=1e-5, decay=0.01, momentum=0.9, nesterov=True)\n",
    "rmsprop = RMSprop(lr=1e-5, rho=0.9, epsilon=1e-8)\n",
    "model.compile(loss=negative_log_likelihood(E_train), optimizer=sgd)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "####\n",
    "print('Training...')\n",
    "model.fit(X_train, Y_train, batch_size=n_patients_train, epochs=100, shuffle=False)  # Shuffle False --> Important!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hr_pred = model.predict(X_train)\n",
    "hr_pred = np.exp(hr_pred)\n",
    "ci = lifelines.utils.concordance_index(Y_train, -hr_pred, E_train)\n",
    "\n",
    "hr_pred2 = model.predict(X_val)\n",
    "hr_pred2 = np.exp(hr_pred2)\n",
    "ci2 = lifelines.utils.concordance_index(Y_val, -hr_pred2, E_val)\n",
    "\n",
    "print(f\"Concordance Index for training dataset: {ci}\")\n",
    "print(f\"Concordance Index for test dataset: {ci2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cox Fitting\n",
    "cph = lifelines.CoxPHFitter()\n",
    "cph.fit(rossi_dataset, 'week', event_col='arrest')\n",
    "\n",
    "cph.print_summary(decimals=3, style='ascii')  # access the results using cf.summary"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
