{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, we will change a few settings to make the notebook look a bit prettier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style> body {font-family: \"Calibri\", cursive, sans-serif;} </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 01 - Using DeepSurvK\n",
    "In this notebook, I will show DeepSurvK's basic functionality.\n",
    "\n",
    "Before going forward, I recommend you check the previous notebook,\n",
    "[\"Understanding DeepSurv\"](./00_understanding_deepsurv.ipynb). \n",
    "There, you will learn the working principles of the original DeepSurv \n",
    "algorithm. Furthermore, there are also some useful usage recommendations \n",
    "that also apply here. However, I will just mention them without going\n",
    "into the details.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import deepsurvk\n",
    "from deepsurvk.datasets import load_whas\n",
    "\n",
    "import logzero\n",
    "from logzero import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "For convenience, DeepSurvK comes with the original datasets. \n",
    "This way, we can load sample data very easily (notice the import at the\n",
    "top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, E_train, = load_whas(partition='training')\n",
    "X_test, Y_test, E_test = load_whas(partition='testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These `training` and `testing` partitions correspond to the original\n",
    "partitions used in DeepSurv's paper. \n",
    "However, you could also load the complete dataset using\n",
    "`partition='complete'` and partition it as you wish (e.g., using\n",
    "sklearn's [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Calculate important parameters.\n",
    "n_patients_train = X_train.shape[0]\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data\n",
    "Data pre-processing is an important step. However, DeepSurvK leaves this\n",
    "to the user, since it depends very much on the data themselves.\n",
    "As mentioned in the [previous notebook]((./00_understanding_deepsurv.ipynb)), \n",
    "at the very least, I would recommend doing standardization and sorting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)\n",
    "\n",
    "Y_scaler = StandardScaler().fit(Y_train.reshape(-1, 1))\n",
    "Y_train = Y_scaler.transform(Y_train)\n",
    "Y_test = Y_scaler.transform(Y_test)\n",
    "\n",
    "Y_train = Y_train.flatten()\n",
    "Y_test = Y_test.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting\n",
    "sort_idx = np.argsort(Y_train)[::-1]\n",
    "X_train = X_train[sort_idx]\n",
    "Y_train = Y_train[sort_idx]\n",
    "E_train = E_train[sort_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DeepSurvK model\n",
    "When creating an instance of a DeepSurvK model, we can also define its \n",
    "parameters. The only mandatory parameter is `n_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk = deepsurvk.DeepSurvK(n_features=n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since DeepSurvK is just a Keras model, we can take advantage of all the\n",
    "perks and tools that come with it. For example, we can get an overview\n",
    "of the model architecture very easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if you are more the graphical-kind person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(dsk, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define explicitely the (custom) loss function\n",
    "DeepSurv (and therefore DeepSurvK) uses a custom loss function.\n",
    "Namely, it aims to minimize the negative log-likelihood.\n",
    "During the creation of the model, the loss function is defined as a string,\n",
    "which is meant to serve as a place holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsk.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the loss function depends on three parameters:\n",
    "`y_true`, `y_pred`, *and* `E`. Unfortunately, custom loss functions in Keras\n",
    "[need to have their signature (i.e., prototype) as](https://keras.io/api/losses/#creating-custom-losses)\n",
    "`loss_fn(y_true, y_pred)`. Therefore, we need to explicitely define it\n",
    "into the model in a separate step. Fortunately, DeepSurvK makes this very\n",
    "easy by providing the loss function directly. After this, we can just\n",
    "add it to the model by compiling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = deepsurvk.negative_log_likelihood(E_train)\n",
    "dsk.compile(loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "As mentioned earlier, it is practical to use Early Stopping in the\n",
    "case of NaNs in loss values. Additionally, it is also a good idea\n",
    "to use the model that during the training phase yields the lowest loss\n",
    "(which isn't necessarily the one at the end of the training)\n",
    "\n",
    "Both of these practices can be achieved using [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback).\n",
    "DeepSurvK provides a method to generate these callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "callbacks = deepsurvk.common_callbacks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say that you can define your own callbacks as well, of course.\n",
    "\n",
    "## Model fitting\n",
    "After this, we are ready to actually fit our model (as any Keras model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "history = dsk.fit(X_train, Y_train, \n",
    "                  batch_size=n_patients_train,\n",
    "                  epochs=epochs, \n",
    "                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=[5, 5])\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "ax.set_xlabel(\"No. epochs\")\n",
    "ax.set_ylabel(\"Loss [u.a.]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions\n",
    "Finally, we can generate predictions using our model.\n",
    "We can evaluate them using the c-index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_pred_train = np.exp(-dsk.predict(X_train))\n",
    "c_index_train = deepsurvk.concordance_index(Y_train, Y_pred_train, E_train)\n",
    "print(f\"c-index of training dataset = {c_index_train}\")\n",
    "\n",
    "Y_pred_test = np.exp(-dsk.predict(X_test))\n",
    "c_index_test = deepsurvk.concordance_index(Y_test, Y_pred_test, E_test)\n",
    "print(f\"c-index of testing dataset = {c_index_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Optimize parameters using Talos...? https://github.com/autonomio/talos"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent",
    "format_version": "1.3",
    "jupytext_version": "1.5.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
